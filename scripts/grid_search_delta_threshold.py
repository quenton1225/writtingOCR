"""
Delta Threshold ç½‘æ ¼æœç´¢å®éªŒ
===========================

å®éªŒç›®æ ‡ï¼š
    åœ¨æœ€ä¼˜å‚æ•°åŸºç¡€ä¸Š(conf_thresh=0.9, fusion_weight=0.45)ï¼Œ
    æœç´¢æœ€ä½³çš„ delta_threshold å€¼

å®éªŒå‚æ•°ï¼š
    - confidence_threshold: 0.9 (å›ºå®š)
    - fusion_weight: 0.45 (å›ºå®šï¼Œå¯¹åº” bert_weight=0.55)
    - delta_threshold: [-0.5, -0.35, -0.25, -0.15, -0.05, 0, 0.05, 0.15, 0.25, 0.35, 0.5] (11ä¸ªå€¼)

è¾“å‡ºï¼š
    - CSV æŠ¥å‘Š: delta_search_results_YYYYMMDD_HHMMSS.csv
    - å¯è§†åŒ–å›¾è¡¨: delta_search_plots_YYYYMMDD_HHMMSS.png
    - å®éªŒæ€»ç»“: delta_search_summary_YYYYMMDD_HHMMSS.txt

Author: OCR Pipeline Team
Date: 2025-11-29
"""

import sys
import os
from pathlib import Path
from datetime import datetime
import time
import numpy as np
import pandas as pd
import cv2
import re
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from src.custom_ocr import CustomTextRecognizer
from src.custom_ocr.processors import TopKDecoder, CTCDeduplicator, ConfidenceFilter
from src.custom_ocr.processors.grid_context_enhancer import GridContextEnhancer
from src.preprocessing.grid_detection import detect_grid_lines, generate_grid_cells
from src.evaluation import GridAccuracyCalculator


# ==================== é…ç½®åŒº ====================

# å›ºå®šçš„æœ€ä¼˜å‚æ•°
FIXED_CONFIDENCE_THRESHOLD = 0.9
FIXED_FUSION_WEIGHT = 0.45  # å¯¹åº” bert_weight = 0.55

# å®éªŒå‚æ•°: delta_threshold
DELTA_THRESHOLDS = [-0.65, -0.55, -0.45, -0.35, -0.25, -0.15, -0.05, 0.0, 0.05, 0.15, 0.25, 0.35, 0.45, 0.55, 0.65]

# æ•°æ®è·¯å¾„
DATA_CONFIG = {
    'image_path': project_root / 'output' / 'temp_cropped.png',
    'gt_file': project_root / 'data' / 'samples' / '2022 ç¬¬2é¡Œ (å†¬å¥§) (8ä»½)_Original' / 'sample_01_01_ground_truth.txt',
}

# æ¨¡å‹é…ç½®
MODEL_CONFIG = {
    'ocr_model': 'PP-OCRv5_server_rec',
    'bert_model': 'bert-base-chinese',
    'ocr_device': 'gpu:0',
    'bert_device': 'cuda:0',
    'context_window': 10,
}

# è¾“å‡ºé…ç½®
OUTPUT_DIR = project_root / 'output' / 'grid_search'
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# æ—¶é—´æˆ³
TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')


# ==================== è¾…åŠ©å‡½æ•° ====================

def restore_empty_cells(results_list, cells, non_empty_cells):
    """æ¢å¤ç©ºæ ¼å­ï¼Œæ„å»ºå®Œæ•´çš„é¢„æµ‹ç»“æœ"""
    all_cells_dict = {}
    for cell in cells:
        row, col = cell['row'], cell['col']
        all_cells_dict[(row, col)] = {
            'row': row, 'col': col, 'text': '',
            'confidence': 1.0, 'is_empty': True
        }
    
    for i, (result, cell) in enumerate(zip(results_list, non_empty_cells)):
        row, col = cell['row'], cell['col']
        all_cells_dict[(row, col)] = {
            'row': row, 'col': col,
            'text': result.get('text', ''),
            'confidence': result.get('confidence', 0),
            'is_empty': False,
            'ocr_result': result
        }
    
    prediction_data_full = sorted(all_cells_dict.values(), 
                                  key=lambda x: (x['row'], x['col']))
    return prediction_data_full


def reconstruct_cell_data(metrics_dict):
    """ä» metrics é‡æ„æ ¼å­æ•°æ®"""
    pred_cells = []
    gt_cells = []
    
    for row_result in metrics_dict['by_row']:
        row_idx = row_result['row']
        for col_idx, (pred_text, gt_text, conf) in enumerate(
            zip(row_result['pred_cells'], row_result['gt_cells'], 
                row_result['confidences'])
        ):
            pred_cells.append({
                'row': row_idx,
                'col': col_idx,
                'text': pred_text,
                'confidence': conf,
                'is_empty': (pred_text == '')
            })
            gt_cells.append({
                'row': row_idx,
                'col': col_idx,
                'text': gt_text
            })
    
    return pred_cells, gt_cells


def analyze_bert_effects(enhanced_cells, pred_cells, gt_cells):
    """åˆ†æ BERT çš„æ”¹å˜æ•ˆæœ"""
    total_changed = 0
    improved = 0
    degraded = 0
    wrong_to_wrong = 0
    
    for pred, enhanced, gt in zip(pred_cells, enhanced_cells, gt_cells):
        if pred.get('is_empty', False):
            continue
        
        pred_text = pred['text']
        enhanced_text = enhanced['text']
        gt_text = gt['text']
        
        if pred_text != enhanced_text:
            total_changed += 1
            pred_correct = (pred_text == gt_text)
            enhanced_correct = (enhanced_text == gt_text)
            
            if not pred_correct and enhanced_correct:
                improved += 1
            elif pred_correct and not enhanced_correct:
                degraded += 1
            else:
                wrong_to_wrong += 1
    
    return {
        'triggered': total_changed,
        'improved': improved,
        'degraded': degraded,
        'wrong_to_wrong': wrong_to_wrong,
        'net_improvement': improved - degraded,
    }


def run_single_experiment(delta_threshold, baseline_data):
    """è¿è¡Œå•æ¬¡å®éªŒ"""
    print(f"\n{'='*80}")
    print(f"å®éªŒ: delta_threshold={delta_threshold}")
    print(f"{'='*80}")
    
    start_time = time.time()
    
    # è§£åŒ…åŸºçº¿æ•°æ®
    recognizer = baseline_data['recognizer']
    cells = baseline_data['cells']
    non_empty_cells = baseline_data['non_empty_cells']
    cell_images = baseline_data['cell_images']
    results_list = baseline_data['results_list']
    ground_truth = baseline_data['ground_truth']
    calculator = baseline_data['calculator']
    original_metrics = baseline_data['original_metrics']
    
    # åˆ›å»º BERT å¢å¼ºå™¨ (ä½¿ç”¨æ–°çš„ delta_threshold å‚æ•°)
    enhancer = GridContextEnhancer(
        model_name=MODEL_CONFIG['bert_model'],
        device=MODEL_CONFIG['bert_device'],
        context_window=MODEL_CONFIG['context_window'],
        confidence_threshold=FIXED_CONFIDENCE_THRESHOLD,
        fusion_weight=FIXED_FUSION_WEIGHT,
        delta_threshold=delta_threshold,  # ğŸ”¥ å…³é”®å‚æ•°
        verbose=False
    )
    
    # æ‰¹é‡å¢å¼º
    enhanced_results_list = enhancer.enhance_grids(
        grid_results=results_list,
        grid_indices=None  # è‡ªåŠ¨è¯†åˆ«ä½ç½®ä¿¡åº¦æ ¼å­
    )
    
    # ç»Ÿè®¡è§¦å‘æ•°
    triggered_count = sum(
        1 for r in enhanced_results_list
        if r.get('grid_bert_correction', {}).get('corrected', False)
    )
    
    # æ¢å¤ç©ºæ ¼å­
    enhanced_full = restore_empty_cells(enhanced_results_list, cells, non_empty_cells)
    
    # è®¡ç®—å‡†ç¡®ç‡
    enhanced_metrics = calculator.calculate(
        predicted_results=enhanced_full,
        ground_truth=ground_truth,
        align_by_row=True
    )
    
    # é‡æ„æ ¼å­æ•°æ®
    enhanced_cells, _ = reconstruct_cell_data(enhanced_metrics)
    pred_cells, gt_cells = reconstruct_cell_data(original_metrics)
    
    # åˆ†æ BERT æ•ˆæœ
    bert_effects = analyze_bert_effects(enhanced_cells, pred_cells, gt_cells)
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = enhanced_metrics['overall']['accuracy']
    original_accuracy = original_metrics['overall']['accuracy']
    accuracy_gain = accuracy - original_accuracy
    
    non_empty_count = len([c for c in pred_cells if not c.get('is_empty', False)])
    trigger_rate = (bert_effects['triggered'] / non_empty_count * 100) if non_empty_count > 0 else 0
    correction_rate = (bert_effects['improved'] / bert_effects['triggered'] * 100) if bert_effects['triggered'] > 0 else 0
    error_rate = (bert_effects['degraded'] / bert_effects['triggered'] * 100) if bert_effects['triggered'] > 0 else 0
    
    processing_time = time.time() - start_time
    
    # æ‰“å°ç»“æœ
    print(f"\nç»“æœ:")
    print(f"  å‡†ç¡®ç‡: {accuracy:.2f}% (æå‡: {accuracy_gain:+.2f}%)")
    print(f"  è§¦å‘ç‡: {trigger_rate:.2f}% ({bert_effects['triggered']}/{non_empty_count})")
    print(f"  çº æ­£ç‡: {correction_rate:.2f}% ({bert_effects['improved']}ä¸ª)")
    print(f"  é”™è¯¯ç‡: {error_rate:.2f}% ({bert_effects['degraded']}ä¸ª)")
    print(f"  å‡€æ”¹è¿›: {bert_effects['net_improvement']} ä¸ªæ ¼å­")
    print(f"  å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")
    
    # è¿”å›ç»“æœ
    return {
        'delta_threshold': delta_threshold,
        'accuracy': accuracy,
        'accuracy_gain': accuracy_gain,
        'triggered': bert_effects['triggered'],
        'trigger_rate': trigger_rate,
        'improved': bert_effects['improved'],
        'degraded': bert_effects['degraded'],
        'wrong_to_wrong': bert_effects['wrong_to_wrong'],
        'net_improvement': bert_effects['net_improvement'],
        'correction_rate': correction_rate,
        'error_rate': error_rate,
        'processing_time': processing_time,
    }


def prepare_baseline_data():
    """å‡†å¤‡åŸºçº¿æ•°æ® (OCR å’Œæ ¼å­æ£€æµ‹)"""
    print("="*80)
    print("å‡†å¤‡åŸºçº¿æ•°æ®...")
    print("="*80)
    
    # 1. åˆå§‹åŒ– OCR è¯†åˆ«å™¨
    print("\nåˆå§‹åŒ– OCR è¯†åˆ«å™¨...")
    recognizer = CustomTextRecognizer(
        model_name=MODEL_CONFIG['ocr_model'],
        device=MODEL_CONFIG['ocr_device']
    )
    
    # 2. åŠ è½½å›¾åƒå’Œ ground truth
    print("åŠ è½½æµ‹è¯•æ•°æ®...")
    img = cv2.imread(str(DATA_CONFIG['image_path']))
    with open(DATA_CONFIG['gt_file'], 'r', encoding='utf-8') as f:
        ground_truth = f.read()
    
    # 3. æ ¼å­æ£€æµ‹
    print("æ£€æµ‹æ ¼å­...")
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    lines = detect_grid_lines(gray)
    cells = generate_grid_cells(lines['horizontal'], lines['vertical'], img.shape[:2])
    
    # 4. è¿‡æ»¤éç©ºæ ¼å­
    print("è¿‡æ»¤éç©ºæ ¼å­...")
    cell_images = []
    non_empty_cells = []
    for cell in cells:
        x1, y1, x2, y2 = cell['x1'], cell['y1'], cell['x2'], cell['y2']
        cell_img = img[y1:y2, x1:x2]
        if cell_img.size > 0:
            gray_cell = cv2.cvtColor(cell_img, cv2.COLOR_BGR2GRAY)
            h, w = gray_cell.shape
            crop = int(min(h, w) * 0.08)
            if h > 2*crop and w > 2*crop:
                center_region = gray_cell[crop:h-crop, crop:w-crop]
            else:
                center_region = gray_cell
            non_white_ratio = (center_region < 240).sum() / center_region.size
            if non_white_ratio > 0.005:
                cell_images.append(cell_img)
                non_empty_cells.append(cell)
    
    print(f"âœ“ éç©ºæ ¼å­æ•°: {len(cell_images)}")
    
    # 5. OCR è¯†åˆ«
    print("\nOCR è¯†åˆ«æ‰€æœ‰æ ¼å­...")
    decoder = TopKDecoder(k=5)
    deduplicator = CTCDeduplicator()
    conf_filter = ConfidenceFilter(threshold=0.3)
    
    batch_raw_outputs = recognizer.batch_predict_with_raw_output(cell_images)
    results_list = []
    for raw_output in batch_raw_outputs:
        decoded = decoder(raw_output)
        deduped = deduplicator(decoded)
        filtered = conf_filter(deduped)
        results_list.append(filtered)
    
    prediction_data_full = restore_empty_cells(results_list, cells, non_empty_cells)
    
    # 6. è®¡ç®—åŸå§‹å‡†ç¡®ç‡
    print("\nè®¡ç®—åŸå§‹ OCR å‡†ç¡®ç‡...")
    calculator = GridAccuracyCalculator(empty_char='')
    original_metrics = calculator.calculate(
        predicted_results=prediction_data_full,
        ground_truth=ground_truth,
        align_by_row=True
    )
    
    original_accuracy = original_metrics['overall']['accuracy']
    print(f"âœ“ åŸå§‹ OCR å‡†ç¡®ç‡: {original_accuracy:.2f}%")
    
    # è¿”å›æ‰€æœ‰åŸºçº¿æ•°æ®
    return {
        'recognizer': recognizer,
        'cells': cells,
        'non_empty_cells': non_empty_cells,
        'cell_images': cell_images,
        'results_list': results_list,
        'ground_truth': ground_truth,
        'calculator': calculator,
        'original_metrics': original_metrics,
        'original_accuracy': original_accuracy,
    }


def visualize_results(results_df, original_accuracy):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'SimSun']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    # 1. å‡†ç¡®ç‡ vs delta
    ax = axes[0, 0]
    ax.plot(results_df['delta_threshold'], results_df['accuracy'], 
            marker='o', linewidth=2, markersize=10, color='blue')
    ax.axhline(y=original_accuracy, color='red', linestyle='--', 
              label=f'åŸå§‹OCRåŸºçº¿ ({original_accuracy:.2f}%)', linewidth=2)
    best_idx = results_df['accuracy'].idxmax()
    best_row = results_df.iloc[best_idx]
    ax.scatter(best_row['delta_threshold'], best_row['accuracy'],
              c='red', s=300, marker='*', edgecolors='black', linewidths=2,
              label=f'æœ€ä½³: Î´={best_row["delta_threshold"]:.2f}')
    ax.set_xlabel('Delta Threshold')
    ax.set_ylabel('å‡†ç¡®ç‡ (%)')
    ax.set_title('å‡†ç¡®ç‡ vs Delta Threshold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 2. è§¦å‘ç‡ vs delta
    ax = axes[0, 1]
    ax.plot(results_df['delta_threshold'], results_df['trigger_rate'], 
            marker='s', linewidth=2, markersize=10, color='green')
    ax.set_xlabel('Delta Threshold')
    ax.set_ylabel('è§¦å‘ç‡ (%)')
    ax.set_title('BERT è§¦å‘ç‡ vs Delta')
    ax.grid(True, alpha=0.3)
    
    # 3. çº æ­£ç‡å’Œé”™è¯¯ç‡
    ax = axes[0, 2]
    ax.plot(results_df['delta_threshold'], results_df['correction_rate'], 
            marker='o', linewidth=2, label='çº æ­£ç‡', color='green')
    ax.plot(results_df['delta_threshold'], results_df['error_rate'], 
            marker='s', linewidth=2, label='é”™è¯¯å¼•å…¥ç‡', color='red')
    ax.set_xlabel('Delta Threshold')
    ax.set_ylabel('æ¯”ç‡ (%)')
    ax.set_title('çº æ­£ç‡ vs é”™è¯¯å¼•å…¥ç‡')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 4. å‡€æ”¹è¿› vs delta
    ax = axes[1, 0]
    colors = ['red' if x < 0 else 'green' for x in results_df['net_improvement']]
    ax.bar(results_df['delta_threshold'].astype(str), results_df['net_improvement'], 
          color=colors, alpha=0.7, edgecolor='black')
    ax.axhline(y=0, color='black', linestyle='-', linewidth=1)
    ax.set_xlabel('Delta Threshold')
    ax.set_ylabel('å‡€æ”¹è¿›æ ¼å­æ•°')
    ax.set_title('å‡€æ”¹è¿› vs Delta')
    ax.grid(True, alpha=0.3, axis='y')
    
    # 5. æ”¹å¯¹/æ”¹é”™å¯¹æ¯”
    ax = axes[1, 1]
    x = np.arange(len(results_df))
    width = 0.35
    ax.bar(x - width/2, results_df['improved'], width, label='æ”¹å¯¹', color='green', alpha=0.7)
    ax.bar(x + width/2, results_df['degraded'], width, label='æ”¹é”™', color='red', alpha=0.7)
    ax.set_xlabel('Delta Threshold')
    ax.set_ylabel('æ ¼å­æ•°')
    ax.set_title('æ”¹å¯¹ vs æ”¹é”™')
    ax.set_xticks(x)
    ax.set_xticklabels([f'{d:.2f}' for d in results_df['delta_threshold']])
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # 6. ç»¼åˆæ•ˆç‡æŒ‡æ ‡
    ax = axes[1, 2]
    # è®¡ç®—æ•ˆç‡: å‡†ç¡®ç‡æå‡ / è§¦å‘ç‡ (æ¯è§¦å‘1%èƒ½å¸¦æ¥å¤šå°‘å‡†ç¡®ç‡æå‡)
    efficiency = results_df['accuracy_gain'] / (results_df['trigger_rate'] + 0.01)  # é¿å…é™¤0
    ax.plot(results_df['delta_threshold'], efficiency, 
            marker='D', linewidth=2, markersize=10, color='purple')
    ax.set_xlabel('Delta Threshold')
    ax.set_ylabel('æ•ˆç‡ (å‡†ç¡®ç‡æå‡/è§¦å‘ç‡)')
    ax.set_title('BERT ä½¿ç”¨æ•ˆç‡')
    ax.grid(True, alpha=0.3)
    # å°†çºµåæ ‡è®¾ç½®ä¸ºå¯¹æ•°åˆ»åº¦
    ax.set_yscale('log', base=10)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_file = OUTPUT_DIR / f'delta_search_plots_{TIMESTAMP}.png'
    plt.savefig(plot_file, dpi=150, bbox_inches='tight')
    print(f"\nâœ“ å›¾è¡¨å·²ä¿å­˜: {plot_file}")
    
    return fig


def generate_summary(results_df, original_accuracy, total_time):
    """ç”Ÿæˆå®éªŒæ€»ç»“æŠ¥å‘Š"""
    summary_lines = []
    summary_lines.append("="*80)
    summary_lines.append("Delta Threshold ç½‘æ ¼æœç´¢å®éªŒæŠ¥å‘Š")
    summary_lines.append("="*80)
    summary_lines.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    summary_lines.append(f"å®éªŒæ¬¡æ•°: {len(results_df)} æ¬¡")
    summary_lines.append(f"æ€»è€—æ—¶: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
    
    summary_lines.append(f"\n{'='*80}")
    summary_lines.append("ã€å›ºå®šå‚æ•°ã€‘")
    summary_lines.append(f"{'='*80}")
    summary_lines.append(f"ç½®ä¿¡åº¦é˜ˆå€¼: {FIXED_CONFIDENCE_THRESHOLD}")
    summary_lines.append(f"èåˆæƒé‡ (fusion_weight): {FIXED_FUSION_WEIGHT} (å¯¹åº” bert_weight={1-FIXED_FUSION_WEIGHT:.2f})")
    summary_lines.append(f"åŸå§‹ OCR å‡†ç¡®ç‡: {original_accuracy:.2f}%")
    
    summary_lines.append(f"\n{'='*80}")
    summary_lines.append("ã€æœ€ä½³é…ç½®ã€‘")
    summary_lines.append(f"{'='*80}")
    
    # æœ€é«˜å‡†ç¡®ç‡
    best_acc_idx = results_df['accuracy'].idxmax()
    best_acc = results_df.iloc[best_acc_idx]
    summary_lines.append(f"\n1. æœ€é«˜å‡†ç¡®ç‡:")
    summary_lines.append(f"   Delta: {best_acc['delta_threshold']:.2f}")
    summary_lines.append(f"   å‡†ç¡®ç‡: {best_acc['accuracy']:.2f}%")
    summary_lines.append(f"   æå‡: {best_acc['accuracy_gain']:+.2f}%")
    summary_lines.append(f"   å‡€æ”¹è¿›: {best_acc['net_improvement']} ä¸ªæ ¼å­")
    summary_lines.append(f"   è§¦å‘ç‡: {best_acc['trigger_rate']:.2f}%")
    summary_lines.append(f"   é”™è¯¯ç‡: {best_acc['error_rate']:.2f}%")
    
    # æœ€å¤§å‡€æ”¹è¿›
    best_net_idx = results_df['net_improvement'].idxmax()
    best_net = results_df.iloc[best_net_idx]
    summary_lines.append(f"\n2. æœ€å¤§å‡€æ”¹è¿›:")
    summary_lines.append(f"   Delta: {best_net['delta_threshold']:.2f}")
    summary_lines.append(f"   å‡€æ”¹è¿›: {best_net['net_improvement']} ä¸ªæ ¼å­")
    summary_lines.append(f"   å‡†ç¡®ç‡: {best_net['accuracy']:.2f}%")
    summary_lines.append(f"   æå‡: {best_net['accuracy_gain']:+.2f}%")
    
    # æœ€ä½é”™è¯¯ç‡
    best_err_idx = results_df['error_rate'].idxmin()
    best_err = results_df.iloc[best_err_idx]
    summary_lines.append(f"\n3. æœ€ä½é”™è¯¯å¼•å…¥ç‡:")
    summary_lines.append(f"   Delta: {best_err['delta_threshold']:.2f}")
    summary_lines.append(f"   é”™è¯¯ç‡: {best_err['error_rate']:.2f}%")
    summary_lines.append(f"   å‡†ç¡®ç‡: {best_err['accuracy']:.2f}%")
    summary_lines.append(f"   å‡€æ”¹è¿›: {best_err['net_improvement']} ä¸ªæ ¼å­")
    
    summary_lines.append(f"\n{'='*80}")
    summary_lines.append("ã€å®Œæ•´ç»“æœè¡¨ã€‘")
    summary_lines.append(f"{'='*80}")
    summary_lines.append(f"{'Delta':<10} {'å‡†ç¡®ç‡':<10} {'æå‡':<10} {'è§¦å‘ç‡':<10} {'çº æ­£ç‡':<10} {'é”™è¯¯ç‡':<10} {'å‡€æ”¹è¿›':<10}")
    summary_lines.append("-"*80)
    for _, row in results_df.iterrows():
        summary_lines.append(
            f"{row['delta_threshold']:<10.2f} {row['accuracy']:<10.2f}% "
            f"{row['accuracy_gain']:<10.2f}% {row['trigger_rate']:<10.2f}% "
            f"{row['correction_rate']:<10.2f}% {row['error_rate']:<10.2f}% "
            f"{row['net_improvement']:<10.0f}"
        )
    
    summary_lines.append(f"\n{'='*80}")
    summary_lines.append("ã€å…³é”®å‘ç°ã€‘")
    summary_lines.append(f"{'='*80}")
    
    # åˆ†æè¶‹åŠ¿
    if results_df['accuracy'].is_monotonic_increasing:
        summary_lines.append("â€¢ Delta è¶Šå¤§,å‡†ç¡®ç‡è¶Šé«˜ (æ­£ç›¸å…³)")
    elif results_df['accuracy'].is_monotonic_decreasing:
        summary_lines.append("â€¢ Delta è¶Šå¤§,å‡†ç¡®ç‡è¶Šä½ (è´Ÿç›¸å…³)")
    else:
        summary_lines.append("â€¢ å‡†ç¡®ç‡å‘ˆç°éå•è°ƒå…³ç³»,å­˜åœ¨æœ€ä¼˜ç‚¹")
    
    # è§¦å‘ç‡å˜åŒ–
    trigger_diff = results_df['trigger_rate'].max() - results_df['trigger_rate'].min()
    summary_lines.append(f"â€¢ è§¦å‘ç‡å˜åŒ–èŒƒå›´: {results_df['trigger_rate'].min():.2f}% ~ {results_df['trigger_rate'].max():.2f}% (Î”={trigger_diff:.2f}%)")
    
    # é”™è¯¯ç‡è¶‹åŠ¿
    if results_df['error_rate'].corr(results_df['delta_threshold']) > 0.5:
        summary_lines.append("â€¢ Delta è¶Šå¤§,é”™è¯¯å¼•å…¥ç‡è¶Šé«˜")
    elif results_df['error_rate'].corr(results_df['delta_threshold']) < -0.5:
        summary_lines.append("â€¢ Delta è¶Šå¤§,é”™è¯¯å¼•å…¥ç‡è¶Šä½")
    
    summary_lines.append(f"\n{'='*80}")
    summary_lines.append("å®éªŒå®Œæˆ")
    summary_lines.append(f"{'='*80}")
    
    summary_text = '\n'.join(summary_lines)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    summary_file = OUTPUT_DIR / f'delta_search_summary_{TIMESTAMP}.txt'
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(summary_text)
    
    print(summary_text)
    print(f"\nâœ“ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")
    
    return summary_text


def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("Delta Threshold ç½‘æ ¼æœç´¢å®éªŒ")
    print("="*80)
    print(f"\nå›ºå®šå‚æ•°:")
    print(f"  confidence_threshold: {FIXED_CONFIDENCE_THRESHOLD}")
    print(f"  fusion_weight: {FIXED_FUSION_WEIGHT} (bert_weight={1-FIXED_FUSION_WEIGHT:.2f})")
    print(f"\nå®éªŒå‚æ•°:")
    print(f"  delta_threshold: {DELTA_THRESHOLDS}")
    print(f"  å®éªŒæ¬¡æ•°: {len(DELTA_THRESHOLDS)}")
    
    # å‡†å¤‡åŸºçº¿æ•°æ®
    start_time = time.time()
    baseline_data = prepare_baseline_data()
    original_accuracy = baseline_data['original_accuracy']
    
    # è¿è¡Œå®éªŒ
    results = []
    for delta in tqdm(DELTA_THRESHOLDS, desc="è¿è¡Œå®éªŒ"):
        result = run_single_experiment(delta, baseline_data)
        results.append(result)
    
    # è½¬æ¢ä¸º DataFrame
    results_df = pd.DataFrame(results)
    
    # ä¿å­˜ CSV
    csv_file = OUTPUT_DIR / f'delta_search_results_{TIMESTAMP}.csv'
    results_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ CSV ç»“æœå·²ä¿å­˜: {csv_file}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    visualize_results(results_df, original_accuracy)
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    total_time = time.time() - start_time
    generate_summary(results_df, original_accuracy, total_time)
    
    print("\n" + "="*80)
    print("æ‰€æœ‰å®éªŒå®Œæˆï¼")
    print("="*80)


if __name__ == '__main__':
    main()
